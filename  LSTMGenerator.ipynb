{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespearean Sonnet Generator using LSTM Networks\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "In the realm of literature, few names evoke the same level of reverence and admiration as William Shakespeare. His mastery of language and poetic expression has left an indelible mark on the world of literature for centuries. One of the most beloved forms of his work is the sonnet, a 14-line poetic form that Shakespeare employed with unparalleled eloquence.\n",
    "\n",
    "In this project, we delve into the realm of artificial intelligence and natural language processing to create a tool that emulates Shakespeare's sonnet-writing prowess. Leveraging the power of LSTM (Long Short-Term Memory) networks, a type of recurrent neural network well-suited for sequence modeling, we aim to generate new sonnets that echo the style and cadence of the Bard himself.\n",
    "\n",
    "Our journey begins with a corpus of Shakespearean sonnets, from which our model learns the intricate patterns of language, rhyme, and meter characteristic of these timeless works. With each line of verse ingested, the model hones its understanding of Shakespeare's linguistic idiosyncrasies, paving the way for the creation of original sonnets that bear the hallmark of his genius.\n",
    "\n",
    "As we embark on this literary adventure, we invite you to witness the marriage of art and technology, as we breathe new life into the age-old tradition of sonneteering, guided by the ethereal spirit of William Shakespeare.\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "**Develop a Shakespearean Sonnet Generator:**\n",
    "Create an LSTM-based neural network capable of generating new Shakespearean-style sonnets based on learned patterns from a corpus of existing sonnets.\n",
    "\n",
    "**Optimize Model Performance:**\n",
    "Refine the model to produce sonnets that closely mimic the style, vocabulary, and thematic elements of Shakespeare's original works, aiming for high fidelity and coherence.\n",
    "\n",
    "**Evaluate Sonnet Quality:**\n",
    "Conduct thorough evaluation and qualitative analysis of the generated sonnets to assess their literary merit, coherence, and adherence to Shakespearean conventions. Additionally, compare the generated sonnets with authentic Shakespearean sonnets to measure similarity and authenticity.\n",
    "\n",
    "**Enhance Creativity and Originality:**\n",
    "Explore methods to encourage the model to produce innovative and original sonnets while still maintaining fidelity to Shakespearean style and themes, fostering creativity within the constraints of the project.\n",
    "\n",
    "## Project Workflow\n",
    "\n",
    "1. **Data Collection:**\n",
    "   Gather a comprehensive dataset of Shakespearean sonnets, ensuring diversity in themes, styles, and authors if applicable.\n",
    "\n",
    "2. **Data Preprocessing:**\n",
    "   Process the sonnet data by tokenizing the text, cleaning any noise or irrelevant information, and converting the text into numerical sequences suitable for input into the LSTM model.\n",
    "\n",
    "3. **Model Training:**\n",
    "   Train the LSTM-based neural network using the preprocessed sonnet dataset. Experiment with different model architectures, hyperparameters, and training strategies to optimize the model's ability to generate high-quality sonnets in the style of Shakespeare.\n",
    "\n",
    "4. **Model Evaluation:**\n",
    "   Evaluate the trained model's performance by generating new sonnets and assessing their quality, coherence, and adherence to Shakespearean conventions. Utilize both qualitative and quantitative evaluation methods to gauge the model's effectiveness in producing authentic-sounding sonnets.\n",
    "\n",
    "5. **Iterative Refinement:**\n",
    "   Iterate on the model architecture, hyperparameters, and training data based on the evaluation results to further improve the model's performance and enhance the quality of the generated sonnets.\n",
    "\n",
    "6. **Documentation and Presentation:**\n",
    "   Document the project workflow, including data collection methods, preprocessing techniques, model architecture, training process, evaluation metrics, and results. Present the findings and generated sonnets in a clear and engaging manner to stakeholders and interested parties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Libraries:\n",
    "- **NumPy:** For numerical computing tasks.\n",
    "- **Pandas:** For data manipulation and analysis.\n",
    "- **TensorFlow:** Deep learning framework for building and training neural networks.\n",
    "- **re:** Regular expression library for pattern matching and manipulation.\n",
    "- **Tokenizer and pad_sequences:** For tokenizing text data and padding sequences to ensure uniform length.\n",
    "- **RMSprop Optimizer:** A popular optimizer for updating model weights during training.\n",
    "\n",
    "Let's proceed with data preprocessing and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 13:11:07.951827: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 13:11:08.271828: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 13:11:15.380102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonnet Data Reader\n",
    "\n",
    "**Description:**\n",
    "This code snippet defines a path for a file containing sonnets and reads the data from that file. The file path is specified as `peomeData`, and the content of the file is read using the `open()` function in Python. The `with` statement ensures that the file is properly closed after reading its contents. The variable `data` stores the content of the file, which presumably contains sonnets. This code can be a part of a larger project involving natural language processing (NLP), sentiment analysis, or poetry generation using sonnets as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for file with sonnets\n",
    "peomeData = '/home/alrashidi/Desktop/Peome.txt'\n",
    "\n",
    "# Read the data\n",
    "with open(peomeData) as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol Removal from Sonnet Text\n",
    "\n",
    "**Description:**\n",
    "This code snippet defines a regular expression pattern to match symbols in the sonnet text. The pattern `r'[^\\w\\s]'` matches any character that is not a word character (`\\w`) or whitespace character (`\\s`). This pattern effectively matches any symbol or punctuation marks in the text.\n",
    "\n",
    "The `re.sub()` function is then used to replace all occurrences of symbols with an empty string in the `data` variable, storing the result in `data_`.\n",
    "\n",
    "Finally, the first 10 characters of the text without symbols are printed to the console using `print(data_[:10])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The moon rose \n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match symbols\n",
    "pattern = r'[^\\w\\s]'\n",
    "\n",
    "# Replace symbols with an empty string\n",
    "data_ = re.sub(pattern, '', data)\n",
    "\n",
    "# Print the text without symbols\n",
    "print(data_[:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase Conversion and List Creation\n",
    "\n",
    "**Description:**\n",
    "This code snippet converts the text data stored in the `data` variable to lowercase using the `.lower()` method and then splits it into a list of lines using the `.split(\"\\n\")` method. Each line of the text becomes an element in the `corpus` list.\n",
    "\n",
    "The purpose of converting the text to lowercase is to ensure uniformity in the text data, as it makes it easier to handle and analyze text by disregarding case sensitivity.\n",
    "\n",
    "The resulting `corpus` list contains each line of the sonnet text as a separate element, all in lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lower case and save as a list\n",
    "corpus = data.lower().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the first tow rows ['the moon rose over the bay. i had a lot of feelings.', 'i am taken with the hot animal']\n",
      "Len the data list 2875\n"
     ]
    }
   ],
   "source": [
    "print(f\"Show the first tow rows {corpus[:2]}\")\n",
    "print(f\"Len the data list {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "**Description:**\n",
    "This code snippet initializes a `Tokenizer` object from TensorFlow's `Tokenizer` module. The `Tokenizer` is a utility class used to vectorize a text corpus by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf, etc.\n",
    "\n",
    "The `fit_on_texts()` method is then used to fit the tokenizer on the `corpus` data, which essentially updates the internal vocabulary based on the given text data.\n",
    "\n",
    "Finally, the variable `total_words` is assigned the total number of unique words in the text corpus plus one. The additional `+1` is for accommodating the index `0`, which is reserved for padding.\n",
    "\n",
    "This tokenization process prepares the text data for input into a neural network model by converting each word into a unique integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer on text data\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Tokenizer Word Index\n",
    "\n",
    "**Description:**\n",
    "This code snippet retrieves the word index from the previously initialized tokenizer object and converts it into a list of tuples using the `items()` method. It then slices the list to select the first 15 elements.\n",
    "\n",
    "The purpose of this code is to display a subset of the word index, pairing each word with its corresponding index.\n",
    "\n",
    "Finally, the code prints the tokenizer along with the word-index pairs, showing the mapping of words to their respective indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer:\n",
      "the: 1 and: 2 to: 3 i: 4 of: 5 my: 6 in: 7 that: 8 thy: 9 thou: 10 a: 11 love: 12 you: 13 with: 14 is: 15 "
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert word index to list of tuples and slice\n",
    "word_index_list = list(word_index.items())[:15]\n",
    "\n",
    "# Show the tokenizer\n",
    "print(\"Tokenizer:\")\n",
    "for word, index in word_index_list:\n",
    "    print(f\"{word}: {index}\",end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 3785\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of unique words\n",
    "total_words = len(word_index) + 1\n",
    "print(\"Total unique words:\", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Sequence Generation Function\n",
    "\n",
    "**Description:**\n",
    "This code defines a Python function `n_gram_seqs` that generates a list of n-gram sequences for each line of text in a given corpus. The function takes two arguments:\n",
    "- `corpus`: a list of strings representing lines of text to generate n-grams for.\n",
    "- `tokenizer`: an instance of the Tokenizer class containing the word-index dictionary.\n",
    "\n",
    "The function iterates over each line in the corpus and tokenizes it using the provided tokenizer. It then iterates over each tokenized line and generates n-gram sequences by progressively adding tokens from the beginning of the line. The resulting n-gram sequences are appended to the `input_sequences` list.\n",
    "\n",
    "Finally, the function returns the list of generated n-gram sequences.\n",
    "\n",
    "This function is useful for preparing text data for training sequence models, such as recurrent neural networks (RNNs), by creating input-output pairs based on n-gram sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_seqs(corpus, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a list of n-gram sequences\n",
    "\n",
    "    Args:\n",
    "        corpus (list of string): lines of texts to generate n-grams for\n",
    "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
    "\n",
    "    Returns:\n",
    "        input_sequences (list of int): the n-gram sequences for each line in the corpus\n",
    "    \"\"\"\n",
    "    input_sequences = []\n",
    "\n",
    "    for line in corpus:\n",
    "      token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "      for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    return input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram sequences for first example look like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 487],\n",
       " [1, 487, 318],\n",
       " [1, 487, 318, 319],\n",
       " [1, 487, 318, 319, 1],\n",
       " [1, 487, 318, 319, 1, 1009],\n",
       " [1, 487, 318, 319, 1, 1009, 4],\n",
       " [1, 487, 318, 319, 1, 1009, 4, 150],\n",
       " [1, 487, 318, 319, 1, 1009, 4, 150, 11],\n",
       " [1, 487, 318, 319, 1, 1009, 4, 150, 11, 1592],\n",
       " [1, 487, 318, 319, 1, 1009, 4, 150, 11, 1592, 5],\n",
       " [1, 487, 318, 319, 1, 1009, 4, 150, 11, 1592, 5, 1593]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test  function with one example\n",
    "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
    "\n",
    "print(\"n_gram sequences for first example look like this:\\n\")\n",
    "first_example_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram sequences for next 3 examples look like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4, 69],\n",
       " [4, 69, 594],\n",
       " [4, 69, 594, 14],\n",
       " [4, 69, 594, 14, 1],\n",
       " [4, 69, 594, 14, 1, 488],\n",
       " [4, 69, 594, 14, 1, 488, 1594],\n",
       " [5, 6],\n",
       " [5, 6, 1010],\n",
       " [5, 6, 1010, 1595],\n",
       " [5, 6, 1010, 1595, 3],\n",
       " [5, 6, 1010, 1595, 3, 1596],\n",
       " [5, 6, 1010, 1595, 3, 1596, 6],\n",
       " [5, 6, 1010, 1595, 3, 1596, 6, 743],\n",
       " [2, 37],\n",
       " [2, 37, 98],\n",
       " [2, 37, 98, 595],\n",
       " [2, 37, 98, 595, 23],\n",
       " [2, 37, 98, 595, 23, 4],\n",
       " [2, 37, 98, 595, 23, 4, 1011],\n",
       " [2, 37, 98, 595, 23, 4, 1011, 70]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test  function with a bigger corpus\n",
    "next_3_examples_sequence = n_gram_seqs(corpus[1:4], tokenizer)\n",
    "\n",
    "print(\"n_gram sequences for next 3 examples look like this:\\n\")\n",
    "next_3_examples_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_grams of input_sequences have length: 18581\n",
      "maximum length of sequences is: 27\n"
     ]
    }
   ],
   "source": [
    "# Apply the n_gram_seqs transformation to the whole corpus\n",
    "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
    "\n",
    "# Save max length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
    "print(f\"maximum length of sequences is: {max_sequence_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Padding Function\n",
    "\n",
    "**Description:**\n",
    "This code defines a Python function `pad_seqs` that pads tokenized sequences to ensure they all have the same length. The function takes two arguments:\n",
    "- `input_sequences`: a list of lists containing tokenized sequences to pad.\n",
    "- `maxlen`: an integer specifying the maximum length to pad sequences to.\n",
    "\n",
    "The function uses the `pad_sequences` function from the TensorFlow module to pad each sequence in `input_sequences` to the specified maximum length. The padding is added at the beginning of each sequence (`padding='pre'`).\n",
    "\n",
    "Finally, the function returns an array of padded sequences, where each sequence has the same length specified by `maxlen`.\n",
    "\n",
    "This function is useful for preparing input data for training neural network models, ensuring that input sequences have consistent dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seqs(input_sequences, maxlen):\n",
    "    \"\"\"\n",
    "    Pads tokenized sequences to the same length\n",
    "\n",
    "    Args:\n",
    "        input_sequences (list of int): tokenized sequences to pad\n",
    "        maxlen (int): maximum length of the token sequences\n",
    "\n",
    "    Returns:\n",
    "        padded_sequences (array of int): tokenized sequences padded to the same length\n",
    "    \"\"\"\n",
    "    padded_sequences = pad_sequences(input_sequences, maxlen, padding='pre')\n",
    "\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
       "         487],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,  487,\n",
       "         318],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    1,  487,  318,\n",
       "         319],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    1,  487,  318,  319,\n",
       "           1],\n",
       "       [   0,    0,    0,    0,    0,    0,    1,  487,  318,  319,    1,\n",
       "        1009],\n",
       "       [   0,    0,    0,    0,    0,    1,  487,  318,  319,    1, 1009,\n",
       "           4],\n",
       "       [   0,    0,    0,    0,    1,  487,  318,  319,    1, 1009,    4,\n",
       "         150],\n",
       "       [   0,    0,    0,    1,  487,  318,  319,    1, 1009,    4,  150,\n",
       "          11],\n",
       "       [   0,    0,    1,  487,  318,  319,    1, 1009,    4,  150,   11,\n",
       "        1592],\n",
       "       [   0,    1,  487,  318,  319,    1, 1009,    4,  150,   11, 1592,\n",
       "           5],\n",
       "       [   1,  487,  318,  319,    1, 1009,    4,  150,   11, 1592,    5,\n",
       "        1593]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test  function with the n_grams_seq of the first example\n",
    "first_padded_seq = pad_seqs(first_example_sequence, max([len(x) for x in first_example_sequence]))\n",
    "first_padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    4,   69],\n",
       "       [   0,    0,    0,    0,    0,    4,   69,  594],\n",
       "       [   0,    0,    0,    0,    4,   69,  594,   14],\n",
       "       [   0,    0,    0,    4,   69,  594,   14,    1],\n",
       "       [   0,    0,    4,   69,  594,   14,    1,  488],\n",
       "       [   0,    4,   69,  594,   14,    1,  488, 1594],\n",
       "       [   0,    0,    0,    0,    0,    0,    5,    6],\n",
       "       [   0,    0,    0,    0,    0,    5,    6, 1010],\n",
       "       [   0,    0,    0,    0,    5,    6, 1010, 1595],\n",
       "       [   0,    0,    0,    5,    6, 1010, 1595,    3],\n",
       "       [   0,    0,    5,    6, 1010, 1595,    3, 1596],\n",
       "       [   0,    5,    6, 1010, 1595,    3, 1596,    6],\n",
       "       [   5,    6, 1010, 1595,    3, 1596,    6,  743],\n",
       "       [   0,    0,    0,    0,    0,    0,    2,   37],\n",
       "       [   0,    0,    0,    0,    0,    2,   37,   98],\n",
       "       [   0,    0,    0,    0,    2,   37,   98,  595],\n",
       "       [   0,    0,    0,    2,   37,   98,  595,   23],\n",
       "       [   0,    0,    2,   37,   98,  595,   23,    4],\n",
       "       [   0,    2,   37,   98,  595,   23,    4, 1011],\n",
       "       [   2,   37,   98,  595,   23,    4, 1011,   70]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test  function with the n_grams_seq of the next 3 examples\n",
    "next_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\n",
    "next_3_padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded corpus has shape: (18581, 27)\n"
     ]
    }
   ],
   "source": [
    "# Pad the whole corpus\n",
    "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
    "\n",
    "print(f\"padded corpus has shape: {input_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature and Label Generation Function\n",
    "\n",
    "**Description:**\n",
    "This code defines a Python function `features_and_labels` that generates features and labels from n-grams. The function takes two arguments:\n",
    "- `input_sequences`: a list of lists containing sequences to split features and labels from.\n",
    "- `total_words`: an integer specifying the vocabulary size.\n",
    "\n",
    "The function extracts features from `input_sequences` by selecting all elements except the last one for each sequence, effectively removing the last token, which will be used as the label. The labels are extracted separately as the last token of each sequence.\n",
    "\n",
    "Next, the labels are one-hot encoded using the `to_categorical` function from TensorFlow, with the number of classes set to `total_words`. This converts the integer labels into binary vectors with a 1 at the index corresponding to the label and 0s elsewhere.\n",
    "\n",
    "Finally, the function returns arrays of features and one-hot encoded labels.\n",
    "\n",
    "This function is useful for preparing data for training classification models, where the input consists of sequences of tokens and the output is a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_labels(input_sequences, total_words):\n",
    "    \"\"\"\n",
    "    Generates features and labels from n-grams\n",
    "\n",
    "    Args:\n",
    "        input_sequences (list of int): sequences to split features and labels from\n",
    "        total_words (int): vocabulary size\n",
    "\n",
    "    Returns:\n",
    "        features, one_hot_labels (array of int, array of int): arrays of features and one-hot encoded labels\n",
    "    \"\"\"\n",
    "    features = input_sequences[:,:-1]\n",
    "    labels = input_sequences[:,-1]\n",
    "    one_hot_labels = to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "    return features, one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels have shape: (11, 3785)\n",
      "\n",
      "features look like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,  487],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    1,  487,  318],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    1,  487,  318,  319],\n",
       "       [   0,    0,    0,    0,    0,    0,    1,  487,  318,  319,    1],\n",
       "       [   0,    0,    0,    0,    0,    1,  487,  318,  319,    1, 1009],\n",
       "       [   0,    0,    0,    0,    1,  487,  318,  319,    1, 1009,    4],\n",
       "       [   0,    0,    0,    1,  487,  318,  319,    1, 1009,    4,  150],\n",
       "       [   0,    0,    1,  487,  318,  319,    1, 1009,    4,  150,   11],\n",
       "       [   0,    1,  487,  318,  319,    1, 1009,    4,  150,   11, 1592],\n",
       "       [   1,  487,  318,  319,    1, 1009,    4,  150,   11, 1592,    5]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test  function with the padded n_grams_seq of the first example\n",
    "first_features, first_labels = features_and_labels(first_padded_seq, total_words)\n",
    "\n",
    "print(f\"labels have shape: {first_labels.shape}\")\n",
    "print(\"\\nfeatures look like this:\\n\")\n",
    "first_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features have shape: (18581, 26)\n",
      "labels have shape: (18581, 3785)\n"
     ]
    }
   ],
   "source": [
    "# Split the whole corpus\n",
    "features, labels = features_and_labels(input_sequences, total_words)\n",
    "\n",
    "print(f\"features have shape: {features.shape}\")\n",
    "print(f\"labels have shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Model Creation Function\n",
    "\n",
    "**Description:**\n",
    "This code defines a Python function `create_text_generator_model` that creates a text generation model using a stacked bidirectional LSTM architecture. The function takes the following arguments:\n",
    "- `total_words`: The total number of words in the vocabulary.\n",
    "- `embedding_dim`: The dimension of the word embedding.\n",
    "- `max_sequence_length`: The maximum length of input sequences.\n",
    "- `lstm_units`: The number of units/neurons in the LSTM layers.\n",
    "- `learning_rate`: The learning rate for the RMSprop optimizer.\n",
    "\n",
    "The function creates a Sequential model using TensorFlow's `Sequential` class. The model consists of the following layers:\n",
    "- Embedding Layer: Maps each word index to a dense vector representation.\n",
    "- Stacked Bidirectional LSTM Layers: Two bidirectional LSTM layers with `lstm_units` neurons each, with the first layer returning sequences and the second layer not returning sequences.\n",
    "- Fully Connected Layer: A dense layer with `total_words*2` neurons and ReLU activation function.\n",
    "- Output Layer: A dense layer with `total_words` neurons and softmax activation function, which outputs a probability distribution over the vocabulary.\n",
    "\n",
    "The model is compiled using categorical cross-entropy loss for multi-class classification and the RMSprop optimizer with the specified `learning_rate`.\n",
    "\n",
    "Finally, the function returns the compiled text generation model.\n",
    "\n",
    "This function is useful for creating and compiling a deep learning model for text generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_generator_model(total_words: int, \n",
    "                                embedding_dim: int, \n",
    "                                max_sequence_length: int, \n",
    "                                lstm_units: int, \n",
    "                                learning_rate) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Creates a text generation model using a stacked bidirectional LSTM architecture.\n",
    "\n",
    "    Args:\n",
    "        total_words (int): The total number of words in the vocabulary.\n",
    "        embedding_dim (int): The dimension of the word embedding.\n",
    "        max_sequence_length (int): The maximum length of input sequences.\n",
    "        lstm_units (int): The number of units/neurons in the LSTM layers.\n",
    "        learning_rate (float): The learning rate for the RMSprop optimizer.\n",
    "\n",
    "    Returns:\n",
    "        model (tf.keras.Model): The compiled text generation model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding Layer\n",
    "        tf.keras.layers.Embedding(total_words, embedding_dim, input_shape=[max_sequence_length-1,]),        \n",
    "        # Stacked Bidirectional LSTM Layers\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units,return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units,return_sequences=False)),\n",
    "        \n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        tf.keras.layers.Dense(total_words*2, activation='relu'),   \n",
    "       \n",
    "        # Output Layer\n",
    "        tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Use categorical crossentropy for multi-class classification\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate), \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Model Summary\n",
    "\n",
    "**Description:**\n",
    "This code initializes variables for the embedding dimension, LSTM units, and learning rate. It then creates a text generation model using the `create_text_generator_model` function defined earlier, passing the specified parameters.\n",
    "\n",
    "Finally, it prints a summary of the model architecture using the `summary()` method, which provides a detailed overview of the layers, their output shapes, and the number of parameters in the model.\n",
    "\n",
    "This summary is helpful for understanding the structure of the model and verifying that it matches your expectations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Model Summary\n",
    "\n",
    "**Description:**\n",
    "This code initializes variables for the embedding dimension, LSTM units, and learning rate. It then creates a text generation model using the `create_text_generator_model` function defined earlier, passing the specified parameters.\n",
    "\n",
    "Finally, it prints a summary of the model architecture using the `summary()` method, which provides a detailed overview of the layers, their output shapes, and the number of parameters in the model.\n",
    "\n",
    "This summary is helpful for understanding the structure of the model and verifying that it matches your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100  # embedding dimension\n",
    "lstm_units = 200  # LSTM units\n",
    "learning_rate = 1e-3  # learning rate\n",
    "\n",
    "# Get the  model\n",
    "model = create_text_generator_model(total_words, embedding_dim, max_sequence_len, lstm_units, learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(features, labels, epochs=250, verbose=1, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Metrics Visualization\n",
    "\n",
    "**Description:**\n",
    "This code snippet retrieves the training and validation accuracy and loss metrics from the `history` object obtained after training a neural network model. It then plots these metrics over the number of epochs to visualize the training progress and model performance.\n",
    "\n",
    "The first part retrieves the accuracy and loss metrics (`acc`, `val_acc`, `loss`, `val_loss`) from the `history` object.\n",
    "\n",
    "Next, it defines the number of epochs based on the length of the accuracy metric (`epochs = range(len(acc))`).\n",
    "\n",
    "The code then plots the training and validation accuracy over the epochs using the `plt.plot()` function, with red color representing training accuracy and blue color representing validation accuracy. It adds titles and legends to the plot for better interpretation and calls `plt.show()` to display the plot.\n",
    "\n",
    "Similarly, it plots the training and validation loss over the epochs in a separate plot, again adding titles, legends, and displaying the plot.\n",
    "\n",
    "These visualizations are useful for assessing the training progress, identifying overfitting or underfitting, and determining the effectiveness of the model architecture and training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', label=\"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Using Trained Model\n",
    "\n",
    "**Description:**\n",
    "This code snippet generates text using a trained text generation model. It starts with a seed text (`seed_text`) and generates the specified number of words (`next_words`) based on the probabilities predicted by the model.\n",
    "\n",
    "In each iteration of the loop, the seed text is converted into sequences of tokens using the tokenizer, padded to match the model's input shape, and fed into the model to predict the next word. The predicted word is then appended to the seed text.\n",
    "\n",
    "This process continues for the specified number of words (`next_words`), gradually expanding the generated text.\n",
    "\n",
    "Finally, the generated text is printed to the console, which represents a continuation of the initial seed text based on the learned patterns from the training data.\n",
    "\n",
    "This approach allows for the generation of new text based on the style and patterns learned by the model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed_text = \"Naser my brother\"\n",
    "next_words = 100\n",
    "\n",
    "for _ in range(next_words):\n",
    "    # Convert the text into sequences\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    # Pad the sequences\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    # Get the probabilities of predicting a word\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    # Choose the next word based on the maximum probability\n",
    "    predicted = np.argmax(predicted, axis=-1).item()\n",
    "    # Get the actual word from the word index\n",
    "    output_word = tokenizer.index_word[predicted]\n",
    "    # Append to the current text\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we aimed to create a text generation model using a stacked bidirectional LSTM architecture. We started by preprocessing the text data, tokenizing it, and preparing it for training. The model architecture was designed to learn from the sequence of words and generate new text based on the learned patterns.\n",
    "\n",
    "During the training process, we observed that the model achieved high accuracy on the training data. However, as the training progressed, the validation accuracy plateaued or even started decreasing, indicating potential overfitting. Overfitting occurs when the model learns to memorize the training data instead of generalizing well to unseen data. This is a common challenge in deep learning models, especially when dealing with complex architectures and limited data.\n",
    "\n",
    "To address overfitting and improve the model's performance, several strategies can be considered:\n",
    "- **Regularization techniques:** Adding dropout layers or L2 regularization to the model can help prevent overfitting by introducing noise or penalizing large weights.\n",
    "- **Data augmentation:** Increasing the diversity of the training data by applying transformations such as rotation, scaling, or adding noise can help the model generalize better.\n",
    "- **Model architecture modifications:** Adjusting the complexity of the model architecture, such as reducing the number of parameters or using simpler network structures, can also mitigate overfitting.\n",
    "\n",
    "Additionally, further hyperparameter tuning and experimenting with different learning rates, batch sizes, and optimizer settings can also contribute to improving the model's performance.\n",
    "\n",
    "In conclusion, while we have successfully built a text generation model, further refinement is required to address overfitting and enhance its generalization capabilities. By applying appropriate regularization techniques and fine-tuning the model's architecture and hyperparameters, we can develop a more robust and reliable text generation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/code/japeralrashid/shakespearean-sonnet-generator-using-lstm-networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_CODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
